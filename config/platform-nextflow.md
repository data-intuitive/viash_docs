---
title: "Nextflow platform"
nav_order: 4
parent: Config
---

## Introduction

### Pipelines / Workflows

It’s possible to *convert* a viash component into a NextFlow module.
Viash uses NextFlow’s DSL2 for this, effectively creating *modules* that
can be imported in a `main.nf` pipeline definition that deals with the
*logic* of the pipeline rather than the low-level machinery.

When it comes to this low-level machinery and the way viash creates a
module, we refer to [the step-by-step introduction about
DiFlow](https://www.data-intuitive.com/diflow).

### Parallelization

NextFlow, as any other pipeline platform is able to run tasks in
parallel if the pipeline logic allows for that. In order to keep
different parallel branch unique, we add a unique identifier `id`. This
identifier can be a sample identifier, or a plate id (sequencing),
versions of reference files to consider, etc.

In a pipeline, one is often interested in running some computation on
different datasets, inputs or parts of an input. This means that
obviously we need to keep track of where the input chunks are located.
But more importantly, we can not just simply name an output file because
multiple parallel processes might just overwrite each other’s output
files.

### Output Filenames

Therefore, it’s important to keep output files distinct across different
steps of the pipeline but also between different parallel runs. In order
to assure this, a module will define its own output file name. It is
constructed from 3 ingredients:

1.  The unique `id` of the data going into the process
2.  The name of the current component/task
3.  An extension

The extension is derived from the component configuration if a
`default: ...` attribute is present for the corresponding output
argument. If no such default value is provided, the *name* of the option
is used.

As an example, the following argument for a component `comp`

    name: --log
    type: file
    direction: Output

will result in `<id>.comp.log`, while the following

    name: --log
    type: file
    direction: Output
    default: log.txt

will become in `<id>.comp.txt`.

The config can define a directory as output as well, it will be named
accordingly.

**Remark**: If the output is a directory, `type: file` should still be
used, but the corresponding script/code should take care of writing the
content to that directory.

## Configuration Settings

This is an example of a NextFlow platform definition:

    ...
    platforms:
      ...
      - type: nextflow
        publish: true
        per_id: true
        label: highmem

### `id`

The platform definition can be given a unique identifier, which is
especially useful when multiple `type: nextflow` platform definitions
are present for the same component/module:

-   `id` is optional
-   No default value if used

### `image`, `tag`/`version` and `registry`

If no image attributes are configured, viash will use an auto-generated
image name:

    [<namespace>/]<name>:<version>

This name corresponds to the one given by the Docker platform when a
custom build is required. This means that it will only work when a
docker platform definition is used that applies customizations.

If no customizations are applied to the docker platform, it means that
it uses a simple base image and the auto-generated name above will not
be used. It’s possible to manually configure an image to be used to run
the module in mainly two ways:

1.  By simply adding a `image: <name>:<tag>` as an attribute.
2.  By specifying the different parts of the container name explicitly:
    -   `image` for the image name
    -   `tag` or `version`: a tag for the container is fetched from the
        following (in order):
        1.  If a platform `version` is configured, use this
        2.  If a platform `tag` is configured, use it
        3.  Use the global `version` for this viash config
    -   `registry` if the image should be fetched from a remote registry

To repeat: The auto-generated image name is ideal in cases where a
custom container is built for the Docker platform, this container is
built (or can be fetched from a registry) and used in the NextFlow
platform. If no customizations are applied, one should add the correct
`image: ...` to both the `docker` and `nextflow` platform
configurations.

### `publish`

NextFlow uses the autogenerated `work` dirs to manage process IO under
the hood. In order effectively *output* something one can *publish* the
results a module or step in the pipeline. In order to do this, add
`publish: true` to the config:

-   `publish` is optional
-   Default value is `false`

This attribute simply defines if output of a component should be
published yes or no. The output location has to be provided at pipeline
launch by means of the option `--output ...` or in `nextflow.config`:

    params.output = "..."

By default, a subdirectory is created corresponding to the unique ID
that is passed in the triplet. Let us illustrate this with an example.
The following code snippet uses the value of `--input` as an input of a
workflow. The input can include a wildcard so that multiple samples can
run in parallel. We use the parent directory name
(`.getParent().baseName`) as an identifier for the sample. We pass this
as the first entry of the triplet:

    Channel.fromPath(params.input) \
        | map{ it -> [ it.getParent().baseName , it ] } \
        | map{ it -> [ it[0] , it[1], params ] }
        | ...

Say the resulting sample names are `SAMPLE1` and `SAMPLE2`. The next
step in the pipeline will be published (at least by default) under:

    <output>/SAMPLE1/
    <output>/SAMPLE2/

### `path`

When `publish: true`, this attribute defines where the output is written
*relative* to the `params.output` setting. For example,
`path: processed` in combination with `--output s3://some_bucket/` will
store the output of this component under

    s3://some_bucket/processed/

This attribute gives control over the directory structure of the output.
For example:

    path: raw_data

Or even:

    path: raw_data/bcl

Please note that `per_id` and `path` can be combined.

### `per_id`

When `publish: true`, we already noted that a subdirectory is created
automatically per unique `id`. In order to avoid the creation of this
subdirectory, `per_id: false` can be added to the platform
configuration.

`path` (see above) is used, but if needed an additional subdirectory can
be automatically

In the previous example, given two *samples* `SAMPLE1` and `SAMPLE2`
running in parallel jobs the output would be stored under:

    s3://some_bucket/processed/SAMPLE1/<outputfiles for sample1>
    s3://some_bucket/processed/SAMPLE2/<outputfiles for sample2>

with the attribute `per_id: false` this will become:

    s3://some_bucket/processed/<outputfiles for samples 1 and 2>

Please note that all output files automatically have a unique name
attached to them (see earlier) and so not creating the unique
subdirectory is strictly speaking not necessary to avoid collisions.

-   The attribute is optional
-   The default value is `true`

### `label` / `labels`

When running the module in a cluster context and depending on the
cluster type, [NextFlow allows for attaching
labels](https://www.nextflow.io/docs/latest/process.html#label) to the
process that can later be used as selectors for associating resources to
this process.

In order to attach one label to a process/component, one can use the
`label: ...` attribute, multiple labels can be added using
`labels: [ ..., ... ]` and the two can even be mixed.

In the main `nextflow.config`, one can now use this label:

    process {
      ...
      withLabel: bigmem {
         maxForks = 5
         ...
      }
    }

### `stageInMode`

By default Nextflow will create a symbolic link to the inputs for a
process/module and run the tool at hand using those symbolic links. Some
applications do not cope well with this strategy, in that case the files
should effectively be copied rather than linked to. This can be achieved
by using `stageInMode: copy`…

-   The attribute is optional
-   The default value is `symlink`

## Tips

### How to specify multiple inputs?

If a component deals with just one input file, that input file should be
provided as the second element in the
[DiFlow](https://www.data-intuitive.com/diflow/) triplet. In other
words, if this is the first component in a (sub)workflow, two options
are available:

    Channel.fromPath(<...>).map{ file -> [ <id>, file, params ] }

or

    Channel.from(<...>).map{ filename -> [ <id>, file(filename), params ] }

It is crucial that this second element in the triplet is of type `Path`.

If multiple inputs are to be provided corresponding to the same option
for the underlying process or tool a `List` of `Path` objects can be
provided.

For example, say we ran multiple parallel workflows for a single sample
and want to join the result of that. The way to express this in NextFlow
would be something like:

    concatenate_ = singleSample_ \
        | toList \
        | map{ it -> [ it.collect{ b -> b[0]}, it.collect{ a -> a[1] }, params ]} \
        | concatenate

In other words, we pass a `List` of Path objects to the concatenate
module.

In some cases, multiple input arguments deal with different input files,
for instance `fastq` files and a reference file for mapping and
counting. One can pass this to the concatenation module by means of a
`Map`. This approach, for instance can be used to merge meta information
(from a file) to an `h5ad` file:

        singleSample_ = input_ \
            ...
            | combine(meta_) \
            | map{ id, output, params, meta ->
                [ id, [ "input" : output, "meta" : meta ], params ]
            } \
            | annotate

Where the `meta_` `Channel` points to the meta file to be used.

In other words, we either provide a `List` of Path values or in the case
multiple options take different files we use a `HashMap`.

Remark: Be sure to mark the arguments at hand as being of `type: file`
and `direction: input`.

## Multiple outputs

Internally, DiFlow uses a similar approach to keeping track of outputs
as discussed for inputs. What comes out of a module, however, is
slightly different. Since a `workflow` can not emit a multi-channel
object, we are forced to put all outputs on the same `Channel` and so we
use a `Map` again to distinguish both. This is only done for multiple
outputs, though.

By means of an example: Say a module outputs one file, then the triplet
that is returned from the module looks like this:

    [ <id>, file, params ]

If our tools has two output files, say for instance `outputfile.txt` and
`logfile.txt` (as indicated by the command line for the tool that looks
for instance like this:
`.... --output outputfile.txt --log logfile.txt`), we still get one
`Channel` back, but on that `Channel` there are now two *events* and
those look like this:

    [ <id>, [ output: outputfile.txt ], params ]
    [ <id>, [ log: logfile.txt ], params ]

It’s up to the receiving end of the module to split this downstream. The
implicit workflow defined in all the generated module contains some
example code to that, for instance:

    result \
      | filterOutput \
      | view{ "Output for output: " + it[1] }

    result \
      | filterLog \
      | view{ "Output for log: " + it[1] }

Where the `filterLog` process for instance is defined like so:

    // A process that filters out output from the output Map
    process filterOutput {

      input:
        tuple val(id), val(input), val(_params)
      output:
        tuple val(id), val(output), val(_params)
      when:
        input.keySet().contains("output")
      exec:
        output = input["output"]

    }

Alternatively, one could also use methods on the `Channel` itself:

    result \
      | filter{ it[1].keySet().contains("output") }
      | map{ [ it[0], it[1]["output"], it[2] ] }
      | view{ "Output for log: " + it[1] }

One more option is to use the `branch` or `multiMap` `Channel` forking
operators in NextFlow.
